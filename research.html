<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research by Topic</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research by Topic</h1>
</div>
<h2>Graphical Models: Methods and Theory for Problems Motivated by Neuroscience </h2>
<p>High-dimensional graphical models have been a powerful tool for learning connections or interaction patterns
among a large number of variables, with wide applications such as learning gene expression networks, functional
connectivity in neuroscience, etc. While most prior
work focuses on the case when all variables are measured
simultaneously, one typical challenge in real data sets (e.g., calcium imaging data reflecting neuronal activity) is that 
<b>only certain subsets of variables (e.g., neurons in a small cube) can be measured together, or can be measured sufficiently many times</b>.
To estimate the graph (conditional independence relationship) or certain characteristics of the graph accurately, novel statistical methods and theory 
need to developed. I am actively working on this direction and happy to collaborate on related topics!</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/pdf/2210.11625">Graphical Model Inference with Erosely Measured Data</a> <br />
<b>Lili Zheng</b>, Genevera I. Allen <br />
<i>Submitted</i></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2209.08273">Low-Rank Covariance Completion for Graph Quilting with Applications to Functional Connectivity</a> <br />
Andersen Chang, <b>Lili Zheng</b>, Genevera I. Allen <br />
<i>Submitted</i></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9746583">Learning Gaussian Graphical Models with Differing Pairwise Sample Sizes</a> <br />
<b>Lili Zheng</b>, Genevera I. Allen <br />
<i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2022</i></p>
</li>
</ul>
<h2>High-dimensional Networks Estimation in Time Series Models</h2>
<p>High-dimensional autoregressive models can capture how the past events<i>status associated with a huge collection of nodes influence their future events</i>status, where the influence patterns can reveal underlying network structures. For example, the past firing of neurons may trigger or inhibit the future firings of their neighbors; past posts of a twitter user may also influence the likelihood of his/her followers to send new tweets. The influence network among these nodes can then be encoded by the high-dimensional autoregressive parameter. <b>The estimation and testing problem for the underlying network structure</b> imposes both methodological and theoretical challenges.</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2003.07429">Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions</a> <br />
<b>Lili Zheng</b>, Garvesh Raskutti, Rebecca Willett, Benjamin Mark <br />
<i>Journal of Machine Learning Research. 2021</i> [<a href="pre/context-dependent-slides">Slides</a>][<a href="https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models">Code</a>] <br />
In this work, we propose two autoregressive models with corresponding methods and theory for learning 
context-dependent networks that reflect how features associated with an event (such as the content of a social media post) modulate the strength of influences among nodes. The multinomial approach we propose is suited to categorical marks and while the logistic-normal approach is suited to marks with mixed membership in different categories; a mixture approach is also proposed to combine the merits of both methods. The following figure provides a comparison among the three approaches. <br />
<img src="fig/context-dependent-synthetic.png" width="450px" height="560px" alt="alt text" /></p>
</li>
<li><p><a href="https://projecteuclid.org/euclid.ejs/1576119708">Testing for high-dimensional network parameters in auto-regressive models</a> <br />
<b>Lili Zheng</b>, Garvesh Raskutti <br />
<i>Electronic Journal of Statistics. 2019</i> [<a href="https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models">Code</a>] <br />
Below is an example of the hypothesis testing results of our method on Chicago crime data, where the goal is to test which community's past crimes has significant influence upon another community's future crimes. All communities involved in significant edges are colored, showing geographical approximity.<br /></p>
</li>
</ul>
<p><img src="fig/hptest-chicago.png" width="300px" height="400px" alt="alt text" /></p>
<h2>Theoretical Analysis for Stochastic Algorithms</h2>
<p>Stochastic algorithms largely improve the computational efficiency of modern machine learning problems with large data sets. However, much of the established application and theory of stochastic algorithms focus on the case when the loss function is a sum of losses evaluated at each data point, suited for independent samples. <b>When there exists strong correlations among samples (e.g., samples generated from Gaussian processes), can we still apply stochastic gradient descent with convergence guarantees?</b> We investigate this question in the context of learning hyperparameters in Gaussian processes (GPs) and discover a surprisingly positive finding, opening a new, previously unexplored data size regime for GPs.</p>
<p>Papers:</p>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2020/file/1cb524b5a3f3f82be4a7d954063c07e2-Paper.pdf">Stochastic Gradient Descent in Correlated Settings: A Study on Gaussian Processes</a> <br />
Hao Chen*, <b>Lili Zheng</b>*, Raed Al Kontar, Garvesh Raskutti (*: equal contribution)<br />
<i>Accepted in part to Neural Information Processing Systems (NeurIPS). 2020</i> [<a href="pre/SGD-GP_recording.mov">Video</a>][<a href="https://github.com/UMDataScienceLab/SGD-in-Gaussain-processes">Code</a>] <br /></p>
</li>
</ul>
<h2>Tensor Data Analysis</h2>
<p>Tensor data has attracted wide interest in recent years since it contains valuable high-order information,
while its high-dimensionality imposes numerous statistical and computational challenges. One of my research interest is to develop <b>efficient algirithms for solving tenesor problems with strong statistical guarantees</b>. </p>
<p>Papers:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.02482">Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration</a> <br />
Yuchen Zhou, Anru R. Zhang, <b>Lili Zheng</b>, Yazhen Wang <br />
<i>Under revision for IEEE Transactions on Information Theory</i> [<a href="https://github.com/Lili-Zheng-stat/TTOI">Code</a>]<br /></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-11-08 16:07:51 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
