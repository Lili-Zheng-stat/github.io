# jemdoc: menu{menu}{index.html}
= Research by Topic


== Graphical Models: Methods and Theory for Problems Motivated by Neuroscience 
High-dimensional graphical models have been a powerful tool for learning connections or interaction patterns
among a large number of variables, with wide applications such as learning gene expression networks, functional
connectivity in neuroscience, etc. While most prior
work focuses on the case when all variables are measured
simultaneously, one typical challenge in real data sets (e.g., calcium imaging data reflecting neuronal activity) is that 
*only certain subsets of variables (e.g., neurons in a small cube) can be measured together, or can be measured sufficiently many times*.
To estimate the graph (conditional independence relationship) or certain characteristics of the graph accurately, novel statistical methods and theory 
need to developed. I am actively working on this direction and happy to collaborate on related topics!

Papers:
- Learning Gaussian Graphical Models with Differing Pairwise Sample Sizes \n
*Lili Zheng*, Genevera Allen \n
/Submitted/

== High-dimensional Networks Estimation in Time Series Models
High-dimensional autoregressive models can capture how the past events/status associated with a huge collection of nodes influence their future events/status, where the influence patterns can reveal underlying network structures. For example, the past firing of neurons may trigger or inhibit the future firings of their neighbors; past posts of a twitter user may also influence the likelihood of his/her followers to send new tweets. The influence network among these nodes can then be encoded by the high-dimensional autoregressive parameter. *The estimation and testing problem for the underlying network structure* imposes both methodological and theoretical challenges.

Papers:
- [https://arxiv.org/abs/2003.07429 Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions] \n
*Lili Zheng*, Garvesh Raskutti, Rebecca Willett, Benjamin Mark \n
/Journal of Machine Learning Research. 2021/ \[[pre/context-dependent-slides Slides]\]\[[https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models Code]\] \n
[img{400}{500}{alt text} fig/context-dependent-synthetic.png]
- [https://projecteuclid.org/euclid.ejs/1576119708 Testing for high-dimensional network parameters in auto-regressive models] \n
*Lili Zheng*, Garvesh Raskutti \n
/Electronic Journal of Statistics. 2019/ \[[https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models Code]\] \n
[img{300}{500}{alt text} fig/hptest-chicago.png]


== Theoretical Analysis for Stochastic Algorithms
Stochastic algorithms largely improve the computational efficiency of modern machine learning problems with large data sets. However, much of the established application and theory of stochastic algorithms focus on the case when the loss function is a sum of losses evaluated at each data point, suited for independent samples. *When there exists strong correlations among samples (e.g., samples generated from Gaussian processes), can we still apply stochastic gradient descent with convergence guarantees?* We investigate this question in the context of learning hyperparameters in Gaussian processes (GPs) and discover a surprisingly positive finding, opening a new, previously unexplored data size regime for GPs.

Papers:
- [https://papers.nips.cc/paper/2020/file/1cb524b5a3f3f82be4a7d954063c07e2-Paper.pdf Stochastic Gradient Descent in Correlated Settings: A Study on Gaussian Processes] \n
Hao Chen\*, *Lili Zheng*\*, Raed Al Kontar, Garvesh Raskutti (\*: equal contribution)\n
/Accepted in part to Neural Information Processing Systems (NeurIPS). 2020/ \[[pre/SGD-GP_recording.mov Video]\]\[[https://github.com/UMDataScienceLab/SGD-in-Gaussain-processes Code]\] \n


== Tensor Data Analysis
Tensor data has attracted wide interest in recent years since it contains valuable high-order information,
while its high-dimensionality imposes numerous statistical and computational challenges. One of my research interest is to develop *efficient algirithms for solving tenesor problems with strong statistical guarantees*. 

Papers:
- [https://arxiv.org/abs/2010.02482 Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration] \n
Yuchen Zhou, Anru R. Zhang, *Lili Zheng*, Yazhen Wang \n
/Under revision for IEEE Transactions on Information Theory/ \[[https://github.com/Lili-Zheng-stat/TTOI Code]\]\n

