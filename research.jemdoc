# jemdoc: menu{menu}{index.html}
= Research by Topic


== Graphical Models: Methods and Theory for Problems Motivated by Neuroscience 
High-dimensional graphical models have been a powerful tool for learning connections or interaction patterns
among a large number of variables, with wide applications such as learning gene expression networks, functional
connectivity in neuroscience, etc. While most prior
work focuses on the case when all variables are measured
simultaneously, one typical challenge in real data sets (e.g., calcium imaging data reflecting neuronal activity) is that 
*only certain subsets of variables (e.g., neurons in a small cube) can be measured together, or can be measured sufficiently many times*.
To estimate the graph (conditional independence relationship) or certain characteristics of the graph accurately, novel statistical methods and theory 
need to developed. I am actively working on this direction and happy to collaborate on related topics!

Papers:
- [https://arxiv.org/pdf/2210.11625 Graphical Model Inference with Erosely Measured Data] \n
*Lili Zheng*, Genevera I. Allen \n
/Under minor revision at Journal of the American Statistical Association, Theory and Methods/
- [https://arxiv.org/pdf/2209.08273 Low-Rank Covariance Completion for Graph Quilting with Applications to Functional Connectivity] \n
Andersen Chang, *Lili Zheng*, Genevera I. Allen \n
/Under revision at Journal of the American Statistical Association, Applications and Case Studies/
- [https://ieeexplore.ieee.org/abstract/document/9746583 Learning Gaussian Graphical Models with Differing Pairwise Sample Sizes] \n
*Lili Zheng*, Genevera I. Allen \n
/International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2022/

- [https://arxiv.org/abs/2305.13491 Nonparanormal Graph Quilting with Applications to Calcium Imaging] \n
Andersen Chang\*, *Lili Zheng*\*, Gautam Dasarthy, Genevera I. Allen \n
/Under revision at STAT/

== Interpretable Machine Learning
With machine learning models being implemented everywhere in modern life, making them interpretable and trustworthy is a crucial task for researchers from different domains. As a statistician, I am passionate about contributing to the challenging problems in interpretable machine learning through statistical lens, e.g., statistical theory and inference methods.

Papers:
- [https://arxiv.org/abs/2305.13491 Interpretable Machine Learning for Discovery: Statistical Challenges & Opportunities] \n
Genevera I. Allen, Luqin Gan, *Lili Zheng* \n
/Under revision at Annual Review of Statistics and Its Application/

- [https://arxiv.org/abs/2206.02088 Model-Agnostic Confidence Intervals for Feature Importance: A Fast and Powerful Approach Using Minipatch Ensembles] \n
Luqin Gan\*, *Lili Zheng*\*, Genevera I. Allen \n
/Submitted/

== High-dimensional Networks Estimation in Time Series Models
High-dimensional autoregressive models can capture how the past events/status associated with a huge collection of nodes influence their future events/status, where the influence patterns can reveal underlying network structures. For example, the past firing of neurons may trigger or inhibit the future firings of their neighbors; past posts of a twitter user may also influence the likelihood of his/her followers to send new tweets. The influence network among these nodes can then be encoded by the high-dimensional autoregressive parameter. *The estimation and testing problem for the underlying network structure* imposes both methodological and theoretical challenges.

Papers:
- [https://arxiv.org/abs/2003.07429 Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions] \n
*Lili Zheng*, Garvesh Raskutti, Rebecca Willett, Benjamin Mark \n
/Journal of Machine Learning Research. 2021/ \[[pre/context-dependent-slides Slides]\]\[[https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models Code]\] \n
In this work, we propose two autoregressive models with corresponding methods and theory for learning 
context-dependent networks that reflect how features associated with an event (such as the content of a social media post) modulate the strength of influences among nodes. The multinomial approach we propose is suited to categorical marks and while the logistic-normal approach is suited to marks with mixed membership in different categories; a mixture approach is also proposed to combine the merits of both methods. The following figure provides a comparison among the three approaches. \n
[img{450}{560}{alt text} fig/context-dependent-synthetic.png]
- [https://projecteuclid.org/euclid.ejs/1576119708 Testing for high-dimensional network parameters in auto-regressive models] \n
*Lili Zheng*, Garvesh Raskutti \n
/Electronic Journal of Statistics. 2019/ \[[https://github.com/Lili-Zheng-stat/Network-inference-for-autoregressive-models Code]\] \n
Below is an example of the hypothesis testing results of our method on Chicago crime data, where the goal is to test which community's past crimes has significant influence upon another community's future crimes. All communities involved in significant edges are colored, showing geographical approximity.\n

[img{300}{400}{alt text} fig/hptest-chicago.png]


== Tensor Data Analysis
Tensor data has attracted wide interest in recent years since it contains valuable high-order information,
while its high-dimensionality imposes numerous statistical and computational challenges. One of my research interest is to develop *efficient and statistically accurate algirithms for solving real-world tenesor problems*.

Papers:
- [https://ieeexplore.ieee.org/abstract/document/9820079/ A Low-Rank Tensor Completion Approach for Imputing Functional Neuronal Data from Multiple Recordings] \n
*Lili Zheng*; Zachary T. Rewolinski; Genevera I. Allen \n
/IEEE Data Science and Learning Workshop (DSLW). 2022/ \n

- [https://arxiv.org/abs/2010.02482 Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration] \n
Yuchen Zhou, Anru R. Zhang, *Lili Zheng*, Yazhen Wang \n
/IEEE Transactions on Information Theory/ \[[https://github.com/Lili-Zheng-stat/TTOI Code]\]\n


== Non-convex Optimization
Non-convex optimization problems arise frequently from both modern machine learning algorithms (e.g., deep neural networks and Gaussian processes) and complex data structures (missing data). Although being challenging solely from an optimization perspective, these problems can often lend a helping hand from certain statistical modeling. I am interested in the intersection between statistics and optimization, especially when efficient non-convex algorithms can still exhibit strong statistical performance.

Papers:
- [https://arxiv.org/abs/2304.09305 High-dimensional Multi-class Classification with Presence-only Data] \n
*Lili Zheng*, Garvesh Raskutti \n
/Under revision at Electronic Journal of Statistics/ \n

- [https://www.jmlr.org/papers/volume23/20-1365/20-1365.pdf Gaussian Process Parameter
Estimation Using Mini-batch Stochastic Gradient Descent: Convergence Guarantees and Empirical
Benefits] \n
Hao Chen\*, *Lili Zheng*\*, Raed Al Kontar, Garvesh Raskutti (\*: equal contribution)\n
/Journal of Machine Learning Research, 2022; Accepted in part to Neural Information Processing Systems (NeurIPS). 2020/ \[[pre/SGD-GP_recording.mov Video]\]\[[https://github.com/UMDataScienceLab/SGD-in-Gaussain-processes Code]\] \n




